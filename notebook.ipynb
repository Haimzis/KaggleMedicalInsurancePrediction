{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdYI9X-iYUv_"
      },
      "source": [
        "Haim Zisman  \n",
        "Ori Malca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD55BS2gIBcb"
      },
      "source": [
        "# Pre-requirements & Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EppvD_YY0SL"
      },
      "source": [
        "load dataset and dependencies file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ3HJ6bMC3eR"
      },
      "source": [
        "Environment Setup Installation (can be escaped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlBPeI33HH8C"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWde2X6a3Jy9",
        "outputId": "97d83194-59be-414a-b3a0-b595cdfbba56"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'insurance (Python 3.9.19)' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n insurance ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sW-VFkInDGI-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")  # Set seaborn style for plots\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import uniform, randint, norm, zscore, mannwhitneyu, ttest_ind, f_oneway,  probplot, kstest\n",
        "import shap\n",
        "\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opGOh5Rxb6jl"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mh4DroDubzUF"
      },
      "outputs": [],
      "source": [
        "NUMERICAL_FEATURES = ['age', 'bmi', 'children', 'charges']\n",
        "CATEGORICAL_FEATURES = ['sex', 'smoker', 'region']\n",
        "FEATURES = CATEGORICAL_FEATURES + NUMERICAL_FEATURES\n",
        "\n",
        "# Constants\n",
        "LABEL = 'charges'\n",
        "SEED = 0\n",
        "\n",
        "# Visualization Hyperparameters\n",
        "TITLE_FONT_SIZE = 12\n",
        "PLOT_TEXT_SIZE = 11\n",
        "\n",
        "# Label Occurrences\n",
        "FIGURE_SIZE = (6, 4)\n",
        "\n",
        "# Correlation Matrix\n",
        "CORR_FIGURE_SIZE = (12, 6)\n",
        "VALUE_FORMATING = \".2f\"\n",
        "CORR_PALETTE = \"ch:s=-.2,r=.6\"\n",
        "\n",
        "# Training\n",
        "TEST_PORTION = 0.1\n",
        "METRIC_FP_PRECISION = 3\n",
        "METRIC_CRITERIA = 'mean_squared_error'\n",
        "SCIKIT_METRIC_CRITERIA = 'neg_mean_squared_error'\n",
        "NUM_KFOLD = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2tzXjNdyizx"
      },
      "source": [
        "## Metrics Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mUclc-jsyhzG"
      },
      "outputs": [],
      "source": [
        "metrics_dict = {\n",
        "    'rmse': lambda labels, preds: functools.partial(mean_squared_error, squared=False)(labels, preds),\n",
        "    'mean_squared_error': lambda labels, preds: mean_squared_error(labels, preds),\n",
        "    'mean_absolute_error': lambda labels, preds: mean_absolute_error(labels, preds),\n",
        "    'r2_score': lambda labels, preds: r2_score(labels, preds),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLA-naUOcBd9"
      },
      "source": [
        "## Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3IhiDz3Qbu3d"
      },
      "outputs": [],
      "source": [
        "file_path = 'data/insurance.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.insert(0, 'index', df.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdIhc2X3cMOW"
      },
      "source": [
        "# Dataset Analysis\n",
        "\n",
        "The Medical Cost Personal Dataset includes features like `age`, `sex`, `BMI`, `number of children`, `smoking status`, and `region`. The `charges` feature represents individual medical costs billed by insurance. These variables help identify how personal factors like health, lifestyle, and regional differences impact medical expenses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739815OpN5-N"
      },
      "source": [
        "## Data Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT9jGti8-BJD"
      },
      "source": [
        "Data Quality and Nullness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDWoiAiQbwoN",
        "outputId": "cd127af1-cd3a-4206-a62e-16ec5368e496"
      },
      "outputs": [],
      "source": [
        "missing_values = df[FEATURES].isnull().sum()\n",
        "print(\"Missing values in each column:\")\n",
        "print(missing_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARiKpmsNZtC6"
      },
      "source": [
        "Based on the null counts provided for the dataset, it can be concluded that there are no missing values across all features listed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-TYYKh4b12L"
      },
      "source": [
        "duplicated rows check and remove if there are any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTKy7lGxb17c",
        "outputId": "15ac97a0-886d-4b6b-8038-9baff789f68d"
      },
      "outputs": [],
      "source": [
        "duplicate_rows = df[df.duplicated()]\n",
        "print(\"Number of duplicate rows:\", duplicate_rows.shape[0])\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwr9OgJ3cItg"
      },
      "source": [
        "total observations after removing duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "d0I_rPfkjyzC",
        "outputId": "45395cdd-666a-4b01-ecfd-0cd8c7f29f5c"
      },
      "outputs": [],
      "source": [
        "f\"Number of Samples: {df.shape[0]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlxLn0KWcjn4"
      },
      "source": [
        "checking possible values and data type of the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "YhM8g2xqcjtw",
        "outputId": "f13cc03d-18f9-4509-eb7e-e8e3a4a0ae26"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for column in FEATURES:\n",
        "    if df[column].dtype == 'object':\n",
        "        unique_vals = df[column].unique()\n",
        "        possible_values = \"{\" + ', '.join(map(str, unique_vals)) + \"}\"\n",
        "    else:\n",
        "        min_val = df[column].min()\n",
        "        max_val = df[column].max()\n",
        "        possible_values = f\"[{min_val}, {max_val}]\"\n",
        "    data.append([column, possible_values, df[column].dtype])\n",
        "\n",
        "summary_df = pd.DataFrame(data, columns=['Column Name', 'Possible Values', 'Data Type'], index=FEATURES)\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLVExR4wjE77"
      },
      "source": [
        "Our data consists of numeric and categorical measurements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO-SfYQb8fAL"
      },
      "source": [
        "## Quantitative Measure of Skewness for numerical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "kNh90ZbOsKgV",
        "outputId": "58b38c6d-0fcb-4715-9da2-ad0fa1663007"
      },
      "outputs": [],
      "source": [
        "skewness_rate_df = df[NUMERICAL_FEATURES].skew().sort_values(ascending=False)\n",
        "skewness_rate_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wm6GLnxWCcQ"
      },
      "source": [
        "All features are right-skewed in different scales. Further Outliers & Skewness investigation is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmc_PYYf2swQ"
      },
      "source": [
        "## Features Distribution Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk4Hh7pE2xZ7"
      },
      "source": [
        "### Categorical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "JktEpLCGh8hp",
        "outputId": "33cf928c-8c1d-4415-e80e-771286dbcf06"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(12, 4))  # Adjusted to fit only 1 row\n",
        "\n",
        "for i, var in enumerate(CATEGORICAL_FEATURES):\n",
        "    df[var].value_counts().plot.pie(\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        colors=sns.color_palette(\"pastel\"),\n",
        "        ax=axs[i],\n",
        "        textprops={'fontsize': PLOT_TEXT_SIZE}\n",
        "    )\n",
        "    axs[i].set_title(f'Distribution of {var}', fontsize=TITLE_FONT_SIZE)\n",
        "    axs[i].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz5s4bSV21Zw"
      },
      "source": [
        "### Numeric Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EBlCMlwrowo",
        "outputId": "62518f4e-adbe-465e-c3dd-678fbb356a17"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "4RR_zdKZ23xA",
        "outputId": "a8bfb6ac-779c-4048-d543-a73b237691ba"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, len(NUMERICAL_FEATURES), figsize=(15, 3))  # Adjusted to fit only 1 row\n",
        "\n",
        "for i, var in enumerate(NUMERICAL_FEATURES):\n",
        "    sns.histplot(df[var], kde=False, ax=axs[i])\n",
        "    axs[i].set_title(f'Distribution of {var}', fontsize=TITLE_FONT_SIZE)\n",
        "    axs[i].set_xlabel(var, fontsize=PLOT_TEXT_SIZE)\n",
        "    axs[i].set_ylabel('Count', fontsize=PLOT_TEXT_SIZE)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOBh4tTd2OW9"
      },
      "source": [
        "#### Empirical Observations:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. Seems like `bmi` is normally distributed.\n",
        "2. `age` is almost uniformly distributed.\n",
        "3. `charges` is NOT normally distributed. Seems more like an exponential distribution (Log-transformation is applied later to get more of a guassian-like shape and reduce overall skewness).\n",
        "4. `regions` are fairly balanced.\n",
        "5. An imbalance between the two classes of `smoker`, with \"no\" cases being more common than \"yes\".\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc0yzond8IrN"
      },
      "source": [
        "## Numerical Data Distribution Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "FcpKHVlukpi2",
        "outputId": "d129d58d-49f3-4836-a978-02511d4938d2"
      },
      "outputs": [],
      "source": [
        "df[FEATURES].describe().drop('count', axis=0).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imJ9DsBjsxMe"
      },
      "source": [
        "#### Observations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. Data range variability, like in `charges` and `bmi`, suggests potential outliers and the need for normalization.\n",
        "\n",
        "2. Outliers: Significant difference between the Q3 and max value (or min value and Q1) suggest outliers' existence.  \n",
        "Examples:\n",
        "  - `charges` Q3=16657.71, max=63770.42\n",
        "  - `bmi` Q3=34.7, max=53.13 (where std=6.09)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hnUMsdDnZrk"
      },
      "source": [
        "Log-transform `charge` to make it more guassian-like and reduce skewness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1HuuhhMnnZyt"
      },
      "outputs": [],
      "source": [
        "df[LABEL] = df[LABEL].apply(np.log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWdLTlgZAd1n"
      },
      "source": [
        "Let's see how `charges` distribution looks after the transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "sXqUsiRj_nq0",
        "outputId": "7d4e2841-bbf2-44e4-8b98-d5de28b15043"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 1, figsize=FIGURE_SIZE)\n",
        "sns.histplot(df[LABEL], kde=False, ax=axs)\n",
        "axs.set_title('Distribution of Charges', fontsize=TITLE_FONT_SIZE)\n",
        "axs.set_xlabel('Charges', fontsize=PLOT_TEXT_SIZE)\n",
        "axs.set_ylabel('Count', fontsize=PLOT_TEXT_SIZE)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ValUbLzGAlwW"
      },
      "source": [
        "After applying the transformation the `charges` distribution resembles more of a guassian shape, and overall skewness is significantly reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf7qLEEdntBZ"
      },
      "source": [
        "## Normality Check - Numeric Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EEg8Fm0In7h-",
        "outputId": "486516e7-f17f-487c-a39b-8edc4112c050"
      },
      "outputs": [],
      "source": [
        "def check_distribution(df, feature, distribution='norm'):\n",
        "    data = df[feature]\n",
        "\n",
        "    # Visual Inspection: Histogram and Q-Q Plot\n",
        "    plt.figure(figsize=(9, 3))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(data, kde=True)\n",
        "    plt.title(f'Histogram of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if distribution == 'norm':\n",
        "        probplot(data, dist=\"norm\", plot=plt)\n",
        "        dist_name = \"Normal\"\n",
        "    elif distribution == 'uniform':\n",
        "        # Generate uniform Q-Q plot\n",
        "        n = len(data)\n",
        "        theoretical_quantiles = np.linspace(0, 1, n)\n",
        "        sorted_data = np.sort((data - data.min()) / (data.max() - data.min()))  # Normalize to [0, 1]\n",
        "        plt.plot(theoretical_quantiles, sorted_data, 'o', markersize=5, label='Data', color='blue')\n",
        "        plt.plot(theoretical_quantiles, theoretical_quantiles, 'r-', label='Uniform Reference Line')\n",
        "        dist_name = \"Uniform\"\n",
        "    plt.title(f'Q-Q Plot of {feature} against {dist_name} distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistical Test: Kolmogorov-Smirnov Test\n",
        "    if distribution == 'norm':\n",
        "        stat, p_value = kstest(data, 'norm', args=(data.mean(), data.std()))\n",
        "    elif distribution == 'uniform':\n",
        "        stat, p_value = kstest((data - data.min()) / (data.max() - data.min()), 'uniform', args=(0, 1))\n",
        "\n",
        "    print(f'Kolmogorov-Smirnov Test for {feature} against {dist_name} distribution')\n",
        "    print(f'Statistic: {stat}, p-value: {p_value}')\n",
        "    if p_value > 0.05:\n",
        "        print(f'{feature} follows {dist_name} distribution (fail to reject H0)')\n",
        "    else:\n",
        "        print(f'{feature} does not follow {dist_name} distribution (reject H0)')\n",
        "    print()\n",
        "\n",
        "# Example usage\n",
        "# Check normality for 'bmi'\n",
        "check_distribution(df, 'bmi', distribution='norm')\n",
        "\n",
        "# Check uniformity for 'age'\n",
        "check_distribution(df, 'age', distribution='uniform')\n",
        "\n",
        "# Check normality for 'charges'\n",
        "check_distribution(df, 'charges', distribution='norm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw2prUpN4vez"
      },
      "source": [
        "# Regression Problem Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j37j2ivdwfd8"
      },
      "source": [
        "### Objective:\n",
        "\n",
        "Predict **charges** using the following features:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}_i = (\\text{age}, \\text{sex}, \\text{bmi}, \\text{children}, \\text{smoker}, \\text{region})\n",
        "$$\n",
        "\n",
        "Minimize the error between predicted and actual charges:\n",
        "\n",
        "$$\n",
        "\\hat{f} = \\arg\\min_{f} \\text{MSE}(f) = \\arg\\min_{f} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f(\\mathbf{x}_i))^2\n",
        "$$\n",
        "\n",
        "Where the criterion is the **MSE (Mean Squared Error)**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bNdT03lnKuU"
      },
      "source": [
        "# Data Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsOAq0H9Qbyx"
      },
      "source": [
        "### Encode categorical features to numeric representation for Pearson Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IdDAPgSrQXyL"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in CATEGORICAL_FEATURES:\n",
        "    df[column] = label_encoder.fit_transform(df[column])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTlCvhHuNkpu"
      },
      "source": [
        "### Pair-wise Pearson Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "-B3fhdRagdnP",
        "outputId": "2c1686af-3f8b-4cf2-87ed-5abd0aea9337"
      },
      "outputs": [],
      "source": [
        "df[FEATURES]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "g9-mzb4gGRam",
        "outputId": "f7f3b7f3-899f-46f7-816f-5adf049bd730"
      },
      "outputs": [],
      "source": [
        "corr = df[FEATURES].corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "plt.figure(figsize=CORR_FIGURE_SIZE)\n",
        "sns.heatmap(corr, mask=mask, cmap=sns.color_palette(CORR_PALETTE, as_cmap=True), annot=True, fmt=VALUE_FORMATING)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCKO0fMy52G8"
      },
      "source": [
        "#### Observations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. **Key Features**: `smoker` have\n",
        "   significant correlations with `charges`, showing their\n",
        "   importance for the predictive model (there's also some linear correlation to `age` and `bmi`).\n",
        "\n",
        "2. **Weak Correlations**: Features like `region` and\n",
        "   `children` exhibit minimal correlation with `charges`, making them potential candidates for exclusion to simplify the model and possibly enhance performance.\n",
        "\n",
        "3. The low correlation between sex and other features (like charges, age, bmi) suggests that the gender of the person doesn't have a strong linear influence on these variables when encoded numerically.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3SnnxQjOBdv"
      },
      "source": [
        "### Identifying Outliers Across TOP10 Skewed Distributions (Boxplot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbB2cE5MhXZF",
        "outputId": "8b81dc14-263f-4b2e-da2e-84bea8e213e9"
      },
      "outputs": [],
      "source": [
        "# Calculate Z-scores for each numeric column in the DataFrame\n",
        "z_scores = np.abs(stats.zscore(df[['age', 'bmi', 'charges']]))\n",
        "\n",
        "# Set the Z-score threshold (commonly 3, meaning 3 standard deviations from the mean)\n",
        "threshold = 2.75\n",
        "\n",
        "# Identify outliers where the Z-score exceeds the threshold for any feature\n",
        "outliers = df[(z_scores > threshold).any(axis=1)].copy()\n",
        "\n",
        "# Display the outliers\n",
        "outliers[LABEL] = outliers[LABEL].apply(np.exp)\n",
        "print(outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y1mphidSH2V"
      },
      "source": [
        "#### Observations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- **Age, BMI, and children** are the primary features showing outlier values in this dataset.\n",
        "- **BMI values above 47** are consistently outliers in this dataset.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4W5eM8lsJko"
      },
      "source": [
        "### Comparative Kernel Density Estimation: Categorical features' charges distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "tYnrfE0cqFXJ",
        "outputId": "9e6342fe-6588-44ea-cab0-e204192bb18e"
      },
      "outputs": [],
      "source": [
        "NUM_COLUMNS = len(CATEGORICAL_FEATURES)\n",
        "NUM_ROWS = len(CATEGORICAL_FEATURES) // NUM_COLUMNS + 1\n",
        "\n",
        "fig, axes = plt.subplots(NUM_ROWS, NUM_COLUMNS, figsize=(15, 7))\n",
        "\n",
        "for index, feature in enumerate(CATEGORICAL_FEATURES):\n",
        "    row, col = divmod(index, NUM_COLUMNS)\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    # Plot KDE for each category within the feature (e.g., male/female for sex)\n",
        "    for category in df[feature].unique():\n",
        "        sns.kdeplot(df[df[feature] == category][LABEL], ax=ax, label=category, shade=True)\n",
        "\n",
        "    ax.set_title(f'KDE of {LABEL} by {feature}', fontsize=14)\n",
        "    ax.legend(title=feature, fontsize=10)\n",
        "    ax.tick_params(axis='both', labelsize=10)\n",
        "\n",
        "for i in range(index + 1, NUM_ROWS * NUM_COLUMNS):\n",
        "    fig.delaxes(axes.flatten()[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v726_3svZlf5"
      },
      "source": [
        "#### Observations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Highly Influential Feature**:\n",
        "- **Smoker** stands out as the most influential feature due to the clear separation between smokers and non-smokers. This feature has a distinct bimodal distribution for smokers and indicates a strong relationship with higher medical costs.\n",
        "\n",
        "**Weakly Influential Features**:\n",
        "- **Sex** and **region** show significant overlap in their KDE distributions, suggesting these features have limited predictive power regarding medical charges.\n",
        "- These features could add noise to a predictive model, and removing or transforming them might improve the model's performance.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLhRWug_ovXB"
      },
      "source": [
        "## Normality check - (Charges | Categorical values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjSAY0bKo5yx",
        "outputId": "c5a75226-1406-4295-80e4-a5aff01bef50"
      },
      "outputs": [],
      "source": [
        "# Function to check normality using Kolmogorov-Smirnov test\n",
        "def check_normality(df, categorical_columns, continuous_column):\n",
        "    \"\"\"\n",
        "    Check normality of the continuous variable within each category of the categorical variables.\n",
        "\n",
        "    H0: The sample comes from a normal distribution.\n",
        "    H1: The sample does not come from a normal distribution.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame containing the data.\n",
        "    - categorical_columns (list): List of categorical columns to group by.\n",
        "    - continuous_column (str): The continuous variable to check for normality.\n",
        "\n",
        "    Returns:\n",
        "    - results (dict): KS statistics and p-values for each category of each categorical variable.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for cat_col in categorical_columns:\n",
        "        categories = df[cat_col].unique()\n",
        "        cat_results = {}\n",
        "\n",
        "        for category in categories:\n",
        "            subset = df[df[cat_col] == category][continuous_column]\n",
        "            ks_stat, p_value = kstest(subset, 'norm', args=(subset.mean(), subset.std()))\n",
        "            cat_results[category] = {'KS Statistic': ks_stat, 'p-value': p_value}\n",
        "\n",
        "        results[cat_col] = cat_results\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to print normality test results with final interpretation\n",
        "def print_normality_results(normality_results):\n",
        "    \"\"\"\n",
        "    Print the results of the normality tests with final interpretation.\n",
        "\n",
        "    Parameters:\n",
        "    - normality_results (dict): The results from the normality tests.\n",
        "    \"\"\"\n",
        "    for cat_col, cat_results in normality_results.items():\n",
        "        print(f\"Results for categorical column: {cat_col}\")\n",
        "        for category, result in cat_results.items():\n",
        "            ks_stat = result['KS Statistic']\n",
        "            p_value = result['p-value']\n",
        "            print(f\"  Category {category} - KS Statistic: {ks_stat:.4f}, p-value: {p_value:.4f}\")\n",
        "            if p_value > 0.05:\n",
        "                print(f\"  -> {category} follows a normal distribution (fail to reject H0)\")\n",
        "            else:\n",
        "                print(f\"  -> {category} does not follow a normal distribution (reject H0)\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Define categorical and continuous columns\n",
        "categorical_columns = ['smoker', 'region', 'sex']\n",
        "continuous_column = 'charges'\n",
        "\n",
        "# Check normality\n",
        "normality_results = check_normality(df, categorical_columns, continuous_column)\n",
        "\n",
        "# Print results with final interpretation\n",
        "print_normality_results(normality_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJyKBOWvpyRo"
      },
      "source": [
        "# Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "P8s_oTbLqWmM"
      },
      "outputs": [],
      "source": [
        "# Function to perform Mann-Whitney U Test\n",
        "def mann_whitney_u_test(df, col, target):\n",
        "    categories = df[col].unique()\n",
        "    if len(categories) != 2:\n",
        "        raise ValueError(\"Mann-Whitney U Test can only be used with binary categorical variables.\")\n",
        "    group1 = df[df[col] == categories[0]][target]\n",
        "    group2 = df[df[col] == categories[1]][target]\n",
        "    stat, p_value = mannwhitneyu(group1, group2)\n",
        "    print(f\"Results for {col}:\")\n",
        "    print(f\"  Mann-Whitney U Test Statistic: {stat:.4f}, p-value: {p_value}\")\n",
        "    if p_value < 0.05:\n",
        "        print(f\"  -> Reject the null hypothesis: The distribution of {target} is significantly different between {categories[0]} and {categories[1]}.\")\n",
        "    else:\n",
        "        print(f\"  -> Fail to reject the null hypothesis: The distribution of {target} is not significantly different between {categories[0]} and {categories[1]}.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Function to perform ANOVA Test\n",
        "def anova_test(df, col, target):\n",
        "    categories = df[col].unique()\n",
        "    groups = [df[df[col] == category][target] for category in categories]\n",
        "    stat, p_value = f_oneway(*groups)\n",
        "    print(f\"Results for {col}:\")\n",
        "    print(f\"  ANOVA Test Statistic: {stat:.4f}, p-value: {p_value}\")\n",
        "    if p_value < 0.05:\n",
        "        print(f\"  -> Reject the null hypothesis: At least one group mean of {target} is significantly different among {col} categories.\")\n",
        "    else:\n",
        "        print(f\"  -> Fail to reject the null hypothesis: There are no significant differences in group means of {target} among {col} categories.\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Function to perform T-Test\n",
        "def t_test(df, col, target):\n",
        "    categories = df[col].unique()\n",
        "    if len(categories) != 2:\n",
        "        raise ValueError(\"T-Test can only be used with binary categorical variables.\")\n",
        "    group1 = df[df[col] == categories[0]][target]\n",
        "    group2 = df[df[col] == categories[1]][target]\n",
        "    stat, p_value = ttest_ind(group1, group2)\n",
        "    print(f\"Results for {col}:\")\n",
        "    print(f\"  T-Test Statistic: {stat:.4f}, p-value: {p_value}\")\n",
        "    if p_value < 0.05:\n",
        "        print(f\"  -> Reject the null hypothesis: The means of {target} are significantly different between {categories[0]} and {categories[1]}.\")\n",
        "    else:\n",
        "        print(f\"  -> Fail to reject the null hypothesis: The means of {target} are not significantly different between {categories[0]} and {categories[1]}.\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVgSLKQEs_oJ"
      },
      "source": [
        "## Q1: Relationship Between BMI and Medical Charges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "th42cxisqm4K",
        "outputId": "0bd90c5c-630e-4cb5-82c7-ea452b764d31"
      },
      "outputs": [],
      "source": [
        "# Split the data into two groups based on BMI\n",
        "bmi_threshold = 24.9\n",
        "group1 = df[df['bmi'] > bmi_threshold]['charges']\n",
        "group2 = df[df['bmi'] <= bmi_threshold]['charges']\n",
        "\n",
        "# Plot KDE for both groups\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(group1, shade=True, label=f'BMI > {bmi_threshold}', color='blue')\n",
        "sns.kdeplot(group2, shade=True, label=f'BMI <= {bmi_threshold}', color='orange')\n",
        "plt.title('KDE of Charges by BMI Groups')\n",
        "plt.xlabel('Charges')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Check normality of the distributions\n",
        "stat1, p_value1 = kstest(group1, 'norm', args=(group1.mean(), group1.std()))\n",
        "stat2, p_value2 = kstest(group2, 'norm', args=(group2.mean(), group2.std()))\n",
        "\n",
        "print(f'Normality test results for BMI > {bmi_threshold}: KS Statistic = {stat1:.4f}, p-value = {p_value1:.4f}')\n",
        "print(f'Normality test results for BMI <= {bmi_threshold}: KS Statistic = {stat2:.4f}, p-value = {p_value2:.4f}')\n",
        "\n",
        "# Determine which test to use based on normality\n",
        "if p_value1 > 0.05 and p_value2 > 0.05:\n",
        "    # Both groups follow a normal distribution\n",
        "    stat, p_value = ttest_ind(group1, group2)\n",
        "    test_name = 'Independent T-Test'\n",
        "    test_hypothesis = \"The means of charges are equal between the two BMI groups.\"\n",
        "else:\n",
        "    # At least one group does not follow a normal distribution\n",
        "    stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
        "    test_name = 'Mann-Whitney U Test'\n",
        "    test_hypothesis = \"The distributions of charges are equal between the two BMI groups.\"\n",
        "\n",
        "# Output the results\n",
        "print(f'\\n{test_name}: Statistic = {stat:.4f}, p-value = {p_value:.4f}')\n",
        "print(f\"Null Hypothesis: {test_hypothesis}\")\n",
        "\n",
        "# Conclusion based on the test\n",
        "if p_value < 0.05:\n",
        "    print(f\"Conclusion: Reject the null hypothesis. There is a significant difference in charges between the two BMI groups.\")\n",
        "else:\n",
        "    print(f\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in charges between the two BMI groups.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWuMwJFVuVpy"
      },
      "source": [
        "## Q2: Relationship Between Age and Medical Charges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfZULdhOuYf6",
        "outputId": "e167f158-5b2a-40fb-d926-96509ba3efb9"
      },
      "outputs": [],
      "source": [
        "corr[LABEL][corr[LABEL] > 0.15]['age']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lE1bjqnvg2n"
      },
      "source": [
        "**Conclusion**: there is a positive linear relationship between age and charges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2-iBuWPuIv8"
      },
      "source": [
        "## Q3: Relationship Between Region and Medical Charges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn6oB_I4uGxA",
        "outputId": "c96e17cc-779a-4393-ae99-a063cb3e649c"
      },
      "outputs": [],
      "source": [
        "anova_test(df, 'region', 'charges')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEI4y2sZwol5"
      },
      "source": [
        "**Conclusion**: there is a no relationship between region and charges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SBVeH3Xui3w"
      },
      "source": [
        "## Q4: Impact of Smoking on Medical Charges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqXd3tenulY4",
        "outputId": "ef2b7bdf-04d8-4561-eb7b-28358a55c177"
      },
      "outputs": [],
      "source": [
        "mann_whitney_u_test(df, 'smoker', 'charges')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou_Uiwb0w4zv"
      },
      "source": [
        "**Conclusion**: there is a strong, positive linear relationship between region and charges. (according to both correlation heatmap and our statistical test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avX-GD21vH_J"
      },
      "source": [
        "## Extra Question: Relationship Between Sex and Medical Charges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej9tL41CvMdy",
        "outputId": "a3e6edf0-7b97-488c-f62b-a4eafc1319aa"
      },
      "outputs": [],
      "source": [
        "t_test(df, 'sex', 'charges')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz9WkfsixR4t"
      },
      "source": [
        "**Conclusion**: there is no relationship between sex and charges - as we would expect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYYlJoqqfo35"
      },
      "source": [
        "# Baseline Model Training - Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcntRKs1cvT7"
      },
      "source": [
        "shifting from EDA to Linear Regression training with 10-fold cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4v4wbC6No7j"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IO8PPMrF3K_J"
      },
      "outputs": [],
      "source": [
        "def data_preparation(training_df, labels, test_size=TEST_PORTION, random_state=SEED):\n",
        "    X = training_df.values\n",
        "    Y = labels.values\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, Y_train, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYV_dLOqXkR3"
      },
      "source": [
        "Encode categorical features to to one-hot representation for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ezRFp2Tm8j"
      },
      "source": [
        "NOTE: We previously applied a log transformation to the `charges` column to reduce skewness and make the distribution more Gaussian-like, which helps with model convergence. During inference, we'll simply apply the inverse (exponential) function $e^x$ to revert the transformation, as the natural logarithm is bijective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "owNS071lXVAo"
      },
      "outputs": [],
      "source": [
        "onehot_df = df.copy()\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "for column in ['region']:\n",
        "    encoded = encoder.fit_transform(df[[column]])\n",
        "    encoded_col_names = [f\"{column}_{cat}\" for cat in encoder.categories_[0]]\n",
        "    onehot_df = onehot_df.drop(columns=[column]).join(pd.DataFrame(encoded, columns=encoded_col_names, index=df.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "I5pjXb_cvaA1",
        "outputId": "2275c3fb-bfd6-452b-8c3d-5d772df67948"
      },
      "outputs": [],
      "source": [
        "onehot_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN_uN0vRYhhH"
      },
      "source": [
        "Create training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WM4zzP8-vypb"
      },
      "outputs": [],
      "source": [
        "training_df = onehot_df.drop([LABEL], axis=1)\n",
        "labels_df = onehot_df[LABEL]\n",
        "X_train_with_id, X_test_with_id, Y_train, Y_test = data_preparation(training_df, labels_df)\n",
        "X_train, X_test = X_train_with_id[:, 1:], X_test_with_id[:, 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj-Lm0KOIc-i"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jk0pn26oGA7"
      },
      "source": [
        "### Training Loop\n",
        "This loop facilitates the training and evaluation of models through K-Fold cross-validation:\n",
        "\n",
        "1. **Splitting the data**: Data is divided into training and validation sets using K-Fold.\n",
        "2. **Training the model**: A new model is trained for each fold using the training data.\n",
        "3. **Storing models**: Each trained model is saved into a collection for analysis.\n",
        "4. **Evaluating performance**: Metrics are calculated for both training and validation sets.\n",
        "5. **Selecting the best model**: The model with the best validation score is selected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bGrugmftZPz7"
      },
      "outputs": [],
      "source": [
        "def training_loop(X_train, Y_train, kf, metrics_dict, return_only_model=True):\n",
        "    # Initialize lists to store training and test metrics\n",
        "    train_metrics = {metric: [] for metric in metrics_dict.keys()}\n",
        "    test_metrics = {metric: [] for metric in metrics_dict.keys()}\n",
        "    models = []\n",
        "\n",
        "    best_model = None\n",
        "    best_metric_score = float('inf')\n",
        "    best_model_idx = -1\n",
        "\n",
        "    # Perform KFold cross-validation\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "        x_train, x_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_train, y_val = Y_train[train_idx], Y_train[val_idx]\n",
        "\n",
        "        # Initialize and train the model for this fold\n",
        "        model = LinearRegression()\n",
        "        model.fit(x_train, y_train)\n",
        "        models.append(model)  # Save the model for this fold\n",
        "\n",
        "        # Predict on training and validation sets\n",
        "        y_train_pred = model.predict(x_train)\n",
        "        y_val_pred = model.predict(x_val)\n",
        "\n",
        "        # Calculate metrics for both training and validation sets\n",
        "        for metric_name, metric_func in metrics_dict.items():\n",
        "            train_metrics[metric_name].append(metric_func(y_train, y_train_pred))\n",
        "            test_metrics[metric_name].append(metric_func(y_val, y_val_pred))\n",
        "\n",
        "        # Check if this fold's model has the best validation score based on the given metric criteria\n",
        "        if test_metrics[METRIC_CRITERIA][-1] < best_metric_score:\n",
        "            best_metric_score = test_metrics[METRIC_CRITERIA][-1]\n",
        "            best_model = model\n",
        "            best_model_idx = fold_idx\n",
        "\n",
        "    if return_only_model:\n",
        "      return best_model\n",
        "\n",
        "    return best_model, best_model_idx, train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJG_K34s0vF2",
        "outputId": "d19dd20a-a4fd-4cbb-d91b-c3092c1e7a88"
      },
      "outputs": [],
      "source": [
        "kf = KFold(n_splits=NUM_KFOLD, shuffle=True, random_state=SEED)\n",
        "best_model, best_model_idx, train_metrics, test_metrics = training_loop(X_train, Y_train, kf, metrics_dict, return_only_model=False)\n",
        "\n",
        "print(f\"Best model fold: {best_model_idx + 1}\")\n",
        "print(f\"Best {METRIC_CRITERIA} score: {np.min(test_metrics[METRIC_CRITERIA])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN_h7HKlJGvE"
      },
      "source": [
        "## Evaluation Metrics and Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRdVvjvNrrFw"
      },
      "source": [
        "### Plotting Training & Validation Metrics for Best-Fold\n",
        "\n",
        "Ploting a series of graphs for training and validation metrics over different folds.\n",
        "\n",
        "#### Parameters:\n",
        "- evals_results: Contains 'train' and 'val' keys with metric names and epoch values.\n",
        "- metrics: List of metric names to plot (e.g., 'MSE', 'RMSE').\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WnFZ8KrGDREQ"
      },
      "outputs": [],
      "source": [
        "def plot_metrics_row(train_metrics, test_metrics, metrics_names, figsize=(20, 3), lw=2):\n",
        "    fig, axes = plt.subplots(1, len(metrics_names), figsize=figsize)\n",
        "\n",
        "    for ax, metric_name in zip(axes, metrics_names):\n",
        "        folds = range(1, len(train_metrics[metric_name]) + 1)  # Number of folds\n",
        "        ax.plot(folds, train_metrics[metric_name], label=f'{metric_name}_train', color='blue', lw=lw)\n",
        "        ax.plot(folds, test_metrics[metric_name], label=f'{metric_name}_val', color='orange', lw=lw)\n",
        "        ax.set_title(f'{metric_name} Across Folds')\n",
        "        ax.set_ylabel(metric_name)\n",
        "        ax.set_xlabel('Fold')\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "cxFLL1un2q3Z",
        "outputId": "20ce2914-6e4c-455e-db34-935977fbcc15"
      },
      "outputs": [],
      "source": [
        "plot_metrics_row(train_metrics, test_metrics, metrics_dict.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr07N0i46VFo"
      },
      "source": [
        "#### Observations\n",
        "\n",
        "---\n",
        "\n",
        "- **Cross-Fold Variability:** While training is stable, validation metrics show variability in how the model performs on different data splits.\n",
        "- **Generalization:** There is room for improvement in generalization, as validation performance is less consistent, possibly requiring further tuning or regularization.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXyWEafbuoYh"
      },
      "source": [
        "### Summary of best fold performance\n",
        "\n",
        "Summarizing the results over the test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-l88OeAnPACd"
      },
      "outputs": [],
      "source": [
        "def visualize_test_results(metrics_dict, X_test, Y_test, best_model):\n",
        "  rows = []\n",
        "  for metric, func in metrics_dict.items():\n",
        "      # For other metrics, calculate directly\n",
        "      value = func(Y_test, best_model.predict(X_test))\n",
        "      rounded_value = round(value, METRIC_FP_PRECISION)\n",
        "      rows.append([metric, rounded_value])\n",
        "\n",
        "  df_metrics = pd.DataFrame(rows, columns=['Metric', 'Best_Fold_on_Test'])\n",
        "  return df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "LJLj6EU9fDlY",
        "outputId": "58d3ffec-0857-4d64-e892-6615b4d38405"
      },
      "outputs": [],
      "source": [
        "baseline_metrics = visualize_test_results(metrics_dict, X_test, Y_test, best_model)\n",
        "baseline_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ewJBZFuFLh"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oGHxIYFfuFLi"
      },
      "outputs": [],
      "source": [
        "def compare_models(dfs, names, postfix='_Best_Fold'):\n",
        "    modified_dfs = []\n",
        "\n",
        "    for df, name in zip(dfs, names):\n",
        "        renamed_df = df.rename(columns={\"Best_Fold_on_Test\": f\"{name}{postfix}\"})\n",
        "        modified_dfs.append(renamed_df)\n",
        "\n",
        "    comparison_df = modified_dfs[0]\n",
        "    for mod_df in modified_dfs[1:]:\n",
        "        comparison_df = pd.merge(comparison_df, mod_df, on='Metric')\n",
        "\n",
        "    return comparison_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKD1QE8Xm6NI"
      },
      "source": [
        "### Standardization (Z-score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkmpNVt9jqQ6"
      },
      "source": [
        "**Note:** we should avoid applying z-score normalization to categorical features that have undergone one-hot encoding, as this would disrupt the 'binary indicator' relationship where each category is uniquely identified by zeros and ones. This mutual exclusivity is essential for models to correctly interpret the presence or absence of categorical attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "netD2bSJFP6v",
        "outputId": "dfe51f7d-1052-4e9b-ecf8-7df675ae0529"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Make a copy of the training and test data to apply scaling\n",
        "X_train_scaled = np.copy(X_train_with_id)\n",
        "X_test_scaled = np.copy(X_test_with_id)\n",
        "\n",
        "# Extract numerical features without the label and then extract their indices in the dataframe\n",
        "numerical_features = [feature for feature in training_df.columns if feature in NUMERICAL_FEATURES]\n",
        "numerical_indices = [i for i, col in enumerate(training_df.columns) if col in numerical_features]\n",
        "\n",
        "# Fit the scaler only on the numerical columns of the training data\n",
        "scaler.fit(X_train[:, numerical_indices])\n",
        "\n",
        "# Apply the transformation to both train and test sets and only to the numerical columns\n",
        "X_train_scaled[:, numerical_indices] = scaler.transform(X_train_scaled[:, numerical_indices])\n",
        "X_test_scaled[:, numerical_indices] = scaler.transform(X_test_scaled[:, numerical_indices])\n",
        "\n",
        "X_train_scaled = X_train_scaled[:, 1:]\n",
        "X_test_scaled = X_test_scaled[:, 1:]\n",
        "\n",
        "# Train the model with the scaled training data\n",
        "best_model_scaled = training_loop(X_train_scaled, Y_train, kf, metrics_dict)\n",
        "standardized_metrics = visualize_test_results(metrics_dict, X_test_scaled, Y_test, best_model_scaled)\n",
        "\n",
        "# Compare baseline and standardized models\n",
        "standardization_comparison_df = compare_models([baseline_metrics, standardized_metrics], ['Baseline', 'Standardized'])\n",
        "display(standardization_comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CTMdwzldTxh"
      },
      "source": [
        "feature standartization in order to prevent dominance of certain features during model training.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL871xwKuFLk"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEiRUNbcuFLl",
        "outputId": "283c1c7a-c7c1-4e4e-d284-446d49e56522"
      },
      "outputs": [],
      "source": [
        "CHARGES_CORR_THRESHOLD = 0.15\n",
        "corr_with_diagnosis = corr[LABEL]\n",
        "\n",
        "# Identify features with low correlation with the target variable\n",
        "low_corr_features = corr_with_diagnosis[abs(corr_with_diagnosis) < CHARGES_CORR_THRESHOLD].index.tolist()\n",
        "print(\"Dropped features with absolute correlation value less than\", CHARGES_CORR_THRESHOLD, \":\", low_corr_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3nTeHoffuFLm"
      },
      "outputs": [],
      "source": [
        "# Extract feature names in the one-hot encoding representation\n",
        "low_corr_oh_features = [f for f in training_df.columns for f_to_rem in low_corr_features if f.startswith(f_to_rem)]\n",
        "\n",
        "# Get column indices to drop\n",
        "drop_indices = [training_df.columns.get_loc(feature) for feature in low_corr_oh_features]\n",
        "\n",
        "# Remove low-correlation features from the training and test sets\n",
        "FS_X_train = np.delete(X_train_with_id, drop_indices, axis=1)[:, 1:]\n",
        "FS_X_test = np.delete(X_test_with_id, drop_indices, axis=1)[:, 1:]\n",
        "\n",
        "# Train the model using the filtered features\n",
        "best_model_fs = training_loop(FS_X_train, Y_train, kf, metrics_dict)\n",
        "\n",
        "# Evaluate the model on the test set with selected features\n",
        "fs_metrics = visualize_test_results(metrics_dict, FS_X_test, Y_test, best_model_fs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "a9LiDpwOuFLm",
        "outputId": "e46dbf61-07c0-4fa7-8432-6ee084e815a9"
      },
      "outputs": [],
      "source": [
        "fs_comparison_df = compare_models([baseline_metrics, fs_metrics], ['Baseline', 'Feature_Selection'])\n",
        "display(fs_comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1GOYZCruFLn"
      },
      "source": [
        "Feature selection based on exploratory data analysis (EDA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG-R3GPruFLn"
      },
      "source": [
        "## Feature Extraction (Dimentionality Reduction) - PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "78oCwrzRuFLo"
      },
      "outputs": [],
      "source": [
        "# PCA transformation\n",
        "pca = PCA(n_components=3)\n",
        "PCA_X_train = pca.fit_transform(X_train)\n",
        "PCA_X_test = pca.transform(X_test)\n",
        "\n",
        "# Training with PCA-transformed data\n",
        "best_model_pca = training_loop(PCA_X_train, Y_train, kf, metrics_dict)\n",
        "\n",
        "# Evaluate the model on PCA-transformed test data\n",
        "pca_metrics = visualize_test_results(metrics_dict, PCA_X_test, Y_test, best_model_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "VneJWdBruFLo",
        "outputId": "f0bd4fd1-592a-4471-a583-7fe74a232be8"
      },
      "outputs": [],
      "source": [
        "# Compare baseline metrics with PCA-transformed data metrics\n",
        "pca_comparison_df = compare_models([baseline_metrics, pca_metrics], ['Baseline', 'PCA'])\n",
        "display(pca_comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIHNMY_AuFLp"
      },
      "source": [
        "As previously discussed, our dataset has multiple uncorrelated features, using PCA, which creates fewer, orthogonal features from linear combinations of the original ones. This approach offers:\n",
        "\n",
        "- Noise Reduction\n",
        "- Lower Data Complexity\n",
        "- Independent Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzuZtqmxuFLp"
      },
      "source": [
        "## Hard-Outliers Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxnIcumYj9Os",
        "outputId": "3b000e44-a5af-433b-f19f-076dce8aad52"
      },
      "outputs": [],
      "source": [
        "indices_to_remove = outliers.index.values\n",
        "\n",
        "# Apply the removal of outliers on the training and test sets\n",
        "train_mask = ~np.isin(X_train_with_id[:, 0], indices_to_remove)  # Mask for the training set (remove outliers)\n",
        "test_mask = ~np.isin(X_test_with_id[:, 0], indices_to_remove)    # Mask for the test set (remove outliers)\n",
        "\n",
        "# Filter the training and test sets to remove the identified outliers\n",
        "OR_X_train = X_train_with_id[train_mask][:, 1:]  # Remove 'id' column after filtering\n",
        "OR_Y_train = Y_train[train_mask]\n",
        "\n",
        "OR_X_test = X_test_with_id[test_mask][:, 1:]     # Remove 'id' column after filtering\n",
        "OR_Y_test = Y_test[test_mask]\n",
        "\n",
        "print(f\"{len(indices_to_remove)} outliers have been removed from the dataset.\")\n",
        "\n",
        "# Train the model on the data without outliers\n",
        "best_model_or = training_loop(OR_X_train, OR_Y_train, kf, metrics_dict)\n",
        "\n",
        "# Evaluate the model on the test set without outliers\n",
        "outliers_metrics = visualize_test_results(metrics_dict, OR_X_test, OR_Y_test, best_model_or)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "30ig5Bolm3KF",
        "outputId": "90627994-bda9-47d7-e525-24c661d8808c"
      },
      "outputs": [],
      "source": [
        "outliers_comparison_df = compare_models([baseline_metrics, outliers_metrics], ['Baseline', 'Outliers_Removal'])\n",
        "display(outliers_comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7hyYqMyuFLs"
      },
      "source": [
        "The outliers in our dataset represent extreme data points across multiple features. (std > 3)\n",
        "\n",
        "Though not ideal, we can remove these data points and label them as abnormal, as they might not be representative of typical data.\n",
        "\n",
        "To decide which points to remove, we set a threshold based on the number of outlier features in each data point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylJ7twAKuFLs"
      },
      "source": [
        "## Summary\n",
        "\n",
        "For our baseline model, with default configuration:\n",
        "\n",
        "| Method                         | Improvement                |\n",
        "|-------------------------------|----------------------------|\n",
        "| Principal Component Analysis (PCA) | Bad Effect                 |\n",
        "| Feature Selection              | Bad Effect                |\n",
        "| Outlier Removal                 | No Effect                |\n",
        "| Scaling                          | No Effect                   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irMEZn3i_zKs"
      },
      "source": [
        "To improve our best model from HyperParameter fine-tuning, we will combine the top feature engineering methods and test each individually to assess their unique impact on model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjSusGrHF433"
      },
      "source": [
        "# Alternative ML Models Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBAIGfOROfC6"
      },
      "source": [
        "We will explore 3 alternatives for our baseline model, combining hyperparameter tuning.\n",
        "\n",
        "- XGBoost\n",
        "- Random Forest\n",
        "- Support Vector Machine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iryGAyhyJiBd"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWOiB4Y7XqF"
      },
      "source": [
        "   \n",
        "* **Motivation**: To evaluate if a baseline model's performance can be improved through parameter configuration search compared to default settings.  \n",
        "\n",
        "* **Configuration** idea: Employ a RandomizedSearchCV with varied distributions for key hyperparameters, including learning rate, estimators, depth, child weight, gamma, and subsample rate, using cross-validation to identify the best-performing XGBoost configuration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_XIuiYaO8yL"
      },
      "source": [
        "### Parameters Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xYe-GpArKV9O"
      },
      "outputs": [],
      "source": [
        "xgboost_conf = {\n",
        "    'model': XGBRegressor(random_state=SEED),\n",
        "    'param_dict': {\n",
        "        'learning_rate': uniform(0.001, 0.3),\n",
        "        'n_estimators': randint(100, 1000),\n",
        "        'max_depth': randint(1, 10),\n",
        "        'min_child_weight': norm(3, 1),\n",
        "        'gamma': uniform(0, 0.3),\n",
        "        'subsample': uniform(0.7, 0.3),\n",
        "    },\n",
        "    'random_result': None\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHVAILzzJj3P"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oWCPk8U8YtM"
      },
      "source": [
        "* **Motivation**: Random Forest is robust, leveraging an ensemble of decision trees to provide stability and resilience against overfitting.\n",
        "\n",
        "* **Configuration idea**: Utilize RandomizedSearchCV with varied distributions for key hyperparameters in Random Forest, such as the number of estimators, maximum features, depth, samples required for split and leaf, and whether to bootstrap. Use cross-validation to find the optimal configuration for improved accuracy and robustness.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrPodx2oO9Y9"
      },
      "source": [
        "### Parameters Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "j54qZk-yJlMT"
      },
      "outputs": [],
      "source": [
        "rf_conf = {\n",
        "    'model': RandomForestRegressor(random_state=SEED),\n",
        "    'param_dict': {\n",
        "        'n_estimators': randint(100, 500),\n",
        "        'max_features': ['log2'],\n",
        "        'max_depth': randint(1, 6),\n",
        "        'min_samples_split': randint(2, 10),\n",
        "        'min_samples_leaf': randint(1, 5),\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'random_result': None\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ0D_fTaRHf3"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtVBejVS8Hrl"
      },
      "source": [
        "* **Motivation**: SVM is known for its robustness and effectiveness in high-dimensional spaces like ours [30+ features]\n",
        "\n",
        "* **Configuration idea**: Apply RandomizedSearchCV with varied distributions to optimize SVM's hyperparameters, including C, kernel, \\\n",
        "degree, gamma, coef0, shrinking, and tolerance. Utilize cross-validation to identify the optimal SVM configuration for better \\\n",
        "accuracy and generalization.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8f43B7lechY"
      },
      "source": [
        "### Parameters Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pip_R6z7RJ3B"
      },
      "outputs": [],
      "source": [
        "svm_conf = {\n",
        "    'model': SVR(),\n",
        "    'param_dict': {\n",
        "        'C': uniform(0.01, 10),\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'degree': randint(1, 10),\n",
        "        'gamma': ['scale', 'auto'],\n",
        "        'coef0': uniform(0, 10),\n",
        "        'shrinking': [True, False],\n",
        "        'tol': uniform(1e-5, 1e-1),\n",
        "    },\n",
        "    'random_result': None\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1TWtNf6LEVU"
      },
      "source": [
        "## Perform Grid Search For on all of the models (SVM, XGBoost, Random Forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IamsoV5YSOQV"
      },
      "source": [
        "Combine Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Fv9qNQT7Kx8k"
      },
      "outputs": [],
      "source": [
        "gs_conf = {\n",
        "    'SVM': svm_conf,\n",
        "    'XGBoost': xgboost_conf,\n",
        "    'RF': rf_conf\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3rxdNjWSQKm"
      },
      "source": [
        "Run Hyper-Parameter fine-tuning for all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "lCOPwRMLLG9O",
        "outputId": "e1742d8f-05e1-460e-c6ea-7dabc2dca005"
      },
      "outputs": [],
      "source": [
        "# Run hyperparameters finetuning for our chosen models\n",
        "metrics_results = [baseline_metrics]\n",
        "best_configurations = {name: None for name in gs_conf.keys()}\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "for m_name, m_conf in tqdm(gs_conf.items(), total=len(gs_conf), desc='Training models'):\n",
        "    random_search = RandomizedSearchCV(estimator=m_conf['model'], param_distributions=m_conf['param_dict'], scoring=SCIKIT_METRIC_CRITERIA, cv=cv, n_iter=5, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "    m_conf['random_result'] = random_search.fit(X_train, Y_train)\n",
        "    best_configurations[m_name] = m_conf['random_result'].best_params_\n",
        "\n",
        "    # pred on test set using the model with the best parameters\n",
        "    best_model = random_search.best_estimator_\n",
        "    metrics_results.append(visualize_test_results(metrics_dict, X_test, Y_test, best_model))\n",
        "\n",
        "# compare to baseline model\n",
        "models_comparison_df = compare_models(metrics_results, ['Baseline'] + [f'GridSearch_{m_name}' for m_name in gs_conf])\n",
        "print(f'\\nbest models configurations were: {best_configurations}')\n",
        "display(models_comparison_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Rm-NzV5S1-"
      },
      "source": [
        "Based on the following results, the XGBoost model with specified cross-validation performs the best compared to our baseline, SVM, and Random Forest models. With this optimal configuration identified, we'll use it along with effective feature engineering to enhance the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKaghesq8wVT"
      },
      "source": [
        "# Optimal Model and Feature Engineering Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "7_0WCFZWWvDD",
        "outputId": "dadbba89-2258-47ed-a786-1aa521b8b39d"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    'Baseline Data': (X_train, Y_train, X_test, Y_test),\n",
        "    'PCA': (PCA_X_train, Y_train, PCA_X_test, Y_test),\n",
        "    'Hard-Outliers Removal': (OR_X_train, OR_Y_train, OR_X_test, OR_Y_test),\n",
        "    'Feature Selection': (FS_X_train, Y_train, FS_X_test, Y_test),\n",
        "}\n",
        "\n",
        "xgboost_conf = {\n",
        "    'model': XGBRegressor(random_state=SEED),\n",
        "    'param_dict': best_configurations['XGBoost']\n",
        "}\n",
        "\n",
        "metrics_results = []\n",
        "model_dict = {}\n",
        "for dataset_name, (_X_train, _Y_train, _X_test, _Y_test) in tqdm(datasets.items(), total=len(datasets), desc='Training chosen model over datasets'):\n",
        "\n",
        "    # Initialize model with the given configuration\n",
        "    xgboost_model = XGBRegressor(\n",
        "        random_state=xgboost_conf['model'].random_state,\n",
        "        **xgboost_conf['param_dict']\n",
        "    )\n",
        "\n",
        "    # Train the model over all dataset\n",
        "    xgboost_model.fit(_X_train, _Y_train)\n",
        "\n",
        "    model_dict[dataset_name] = xgboost_model\n",
        "\n",
        "    # Visualize test results\n",
        "    metrics_results.append(visualize_test_results(metrics_dict, _X_test, _Y_test, xgboost_model))\n",
        "\n",
        "combination_comparison_df = compare_models(metrics_results, datasets.keys(), postfix='')\n",
        "display(combination_comparison_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "9QC4CgQSW2BT",
        "outputId": "f1aad5f6-e74f-4345-8872-e48baccf31ac"
      },
      "outputs": [],
      "source": [
        "best_dataset = 'Hard-Outliers Removal'\n",
        "_X_train, _Y_train, _X_test, _Y_test = datasets[best_dataset]\n",
        "model = model_dict[best_dataset]\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtQoPdMTbgA6"
      },
      "source": [
        "#### Observations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Based on our comprehensive evaluation of different models\n",
        "\n",
        "1. **Baseline (Linear Regression)**\n",
        "2. **XGBoost + Cross-Validation (CV)**\n",
        "3. **SVM + Cross-Validation (CV)**\n",
        "4. **RandomForest + Cross-Validation (CV)**\n",
        "\n",
        "The best combination to maximize $R^2$ score and minimizes RMSE is **\"XGBoost + CV\"** trained on the baseline dataset, after Outliers-removal. It's worth to mention that, XGBoost can automatically select features.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzehGyV1INxA"
      },
      "source": [
        "# Model Explainability Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7QL5EsQLrqm"
      },
      "source": [
        "Utilizing SHAP (stands for: SHapley Additive exPlanations) is a method in machine learning that helps us understand how our model works. It breaks down each prediction into parts and tells us which features are most important for that prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvdAQZXl2iUt"
      },
      "source": [
        "## SHAP Summary Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY1unTmFZPjn",
        "outputId": "485cb31c-c005-4028-fa61-40cb24bb04fb"
      },
      "outputs": [],
      "source": [
        "training_df.columns[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "Q0-q3ziFLqx0",
        "outputId": "75721929-4d1a-46bb-dcb8-d785b88341e4"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(model)\n",
        "\n",
        "# Restore numpy to df\n",
        "_X_train_df = pd.DataFrame(_X_train, columns=training_df.columns[1:])\n",
        "_X_test_df = pd.DataFrame(_X_test, columns=training_df.columns[1:])\n",
        "\n",
        "shap_values_train = explainer.shap_values(_X_train_df)\n",
        "\n",
        "plt.figure(figsize=(9, 6))\n",
        "shap.summary_plot(shap_values_train, _X_train_df, plot_type='dot', show=False)\n",
        "plt.title(\"SHAP Summary Plot - Training\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "aAPQVowUpizk",
        "outputId": "bc1c4090-a912-4180-a76c-a4eb47200f9d"
      },
      "outputs": [],
      "source": [
        "shap_values_test = explainer.shap_values(_X_test_df)\n",
        "\n",
        "plt.figure(figsize=(9, 6))\n",
        "shap.summary_plot(shap_values_test, _X_test_df, plot_type='dot', show=False)\n",
        "plt.title(\"SHAP Summary Plot - Test\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHdAx6-vbnrn"
      },
      "source": [
        "This SHAP summary plots indicates the impact of various features on the model's predictions. It uses a dot plot to show SHAP values for individual samples, with colors representing the feature's value (blue for low, red for high), and the dots' position indicating the impact on the model's output (left for Benign, right for Malignant).\n",
        "\n",
        "#### Observations\n",
        "---\n",
        "\n",
        "1. **`smoker`**: The most impactful feature, with smokers (red) strongly increasing predictions and non-smokers (blue) decreasing them, consistently across both datasets.\n",
        "\n",
        "2. **`age`**: Older individuals (red) increase predictions, while younger ones (blue) reduce them. The impact is consistent in both sets.\n",
        "\n",
        "3. **`children`** and **`bmi`**: Both show moderate effects. More children and higher BMI values slightly increase predictions.\n",
        "\n",
        "4. **`region`** and **`sex`**: These features have minimal impact, with SHAP values clustering around zero.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4dU34pN2PLU"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "OxM474QP184u",
        "outputId": "e5d980bc-b4dd-428e-9550-064452abd26b"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(explainer(_X_train_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8DISVBF3GED"
      },
      "source": [
        "As we already mentioned in Summary Plot's observations, it seems that the most significant features in decision made were indeed the most correlated features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "ffikOS7Wke9M",
        "outputId": "af44d535-b933-4d70-b8c4-48b0ef1b12db"
      },
      "outputs": [],
      "source": [
        "corr[LABEL][corr[LABEL] > 0.15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaETeEfHqfIr"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNuJa8xHwn0Z"
      },
      "source": [
        "We chose to work with the [Medical Insurance Dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance), which aims to predict insurance cost using 7 numerical & categorical features and a given ground truth (gt).\n",
        "\n",
        "As we analyzed the data, we noticed that some of these features were crucially highly correlated with the target variable.\n",
        "\n",
        "We divided the dataset into two splits:\n",
        "* Training\n",
        "* Test\n",
        "\n",
        "The test split was kept unobserved by the model to mimic real-world scenarios.\n",
        "\n",
        "We used several visualization and analysis methods in our pipeline, such as:\n",
        "\n",
        "* Features Distribution (Pie & Histograms Charts) – Revealed an exponential distribution, which we smoothed using standartization.\n",
        "* Pearson Correlation – Identified less correlated features.\n",
        "* KDE Plots – Revealed distinct patterns in feature distributions.\n",
        "* Outlier Distribution Analysis – Identified hard-outlier data points.\n",
        "\n",
        "Based on these analyses, we employed the following feature engineering techniques:\n",
        "\n",
        "* Feature Selection\n",
        "* Hard-Outliers Removal\n",
        "* Principal Component Analysis\n",
        "\n",
        "We also trained an Linear Regression model with default configuration, achieving fairly good results, with high variance on the test split.\n",
        "\n",
        "We trained three selected models, now with hyperparameter fine-tuning:\n",
        "* Support Vector Machine\n",
        "* Random Forest\n",
        "* XGBoost\n",
        "\n",
        "After training and applying feature engineering, we found that the baseline dataset combined with XGBoost and outliers removal provided the best results and achived great results on the testset.\n",
        "\n",
        "Continuing our analysis, we explored:\n",
        "1. SHAP summary plots for both test and training data to identify and compare key differences.\n",
        "2. Feature importance across the training split.\n",
        "\n",
        "It was intriguing to explore and apply the concepts learned in class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1qst5TqqvyV"
      },
      "source": [
        "## Important Notes\n",
        "- We did not rely on any existing Kaggle notebooks for our analysis.\n",
        "- Most Kaggle notebooks do not conduct thorough error analysis or extensive Exploratory Data Analysis (EDA) as we did.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
